{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce4cdcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f15505d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(filename, chunk_length):\n",
    "    \"\"\"\n",
    "    Splits the text in the specified file into chunks of the specified length.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The path to the file containing the text to be chunked.\n",
    "        chunk_length (int): The desired length of each text chunk.\n",
    "\n",
    "    Returns:\n",
    "        A list of strings, where each string represents a chunk of text from the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            text = file.read()\n",
    "            chunks = [text[i:i + chunk_length] for i in range(0, len(text), chunk_length)]\n",
    "        return chunks\n",
    "    except IOError:\n",
    "        print(f\"Error: Cannot open or read file {filename}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14e32fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_chunks(chunks, model=\"text-davinci-003\", temperature=0.7, max_tokens=256, top_p=1, frequency_penalty=0, presence_penalty=0):\n",
    "    \"\"\"\n",
    "    Summarizes a list of text chunks using the OpenAI API.\n",
    "\n",
    "    Args:\n",
    "        chunks (list): A list of text chunks to be summarized.\n",
    "        model (str): The name of the OpenAI model to use for summarization (default: \"text-davinci-003\").\n",
    "        temperature (float): The temperature of the sampling distribution for generating text (default: 0.7).\n",
    "        max_tokens (int): The maximum number of tokens to generate in the output text (default: 256).\n",
    "        top_p (float): The cumulative probability threshold for top-p sampling (default: 1).\n",
    "        frequency_penalty (float): The frequency penalty to apply to the output text (default: 0).\n",
    "        presence_penalty (float): The presence penalty to apply to the output text (default: 0).\n",
    "\n",
    "    Returns:\n",
    "        A list of summaries for each text chunk.\n",
    "    \"\"\"\n",
    "    summaries = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            response = openai.Completion.create(\n",
    "                model=model,\n",
    "                prompt=f\"Summarize the following text:\\n\\n{chunk}\\n\",\n",
    "                temperature=temperature,\n",
    "                max_tokens=max_tokens,\n",
    "                top_p=top_p,\n",
    "                frequency_penalty=frequency_penalty,\n",
    "                presence_penalty=presence_penalty\n",
    "            )\n",
    "\n",
    "            summaries.append(response[\"choices\"][0][\"text\"].strip())\n",
    "        except Exception as e:\n",
    "            print(f\"Error: Failed to summarize chunk: {e}\")\n",
    "            summaries.append(\"\")\n",
    "\n",
    "    return summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f5447b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text reflects on how art and writing can be used to resist oppressive forces, as well as to share memories and experiences between people. It encourages writers to continue writing, even in the face of difficult times, as a way to protect and preserve memories, stories, and experiences. It emphasizes the importance of art and writing in times of war, and how it can be used to create something new and bring people together.\n"
     ]
    }
   ],
   "source": [
    "filename = \"/Users/alexulanch/Documents/gornick.txt\"\n",
    "chunk_size = 4000\n",
    "\n",
    "# Split the file into chunks and generate summaries for each chunk\n",
    "chunks = chunk_text(filename, chunk_size)\n",
    "summaries = summarize_chunks(chunks)\n",
    "\n",
    "# Concatenate the summaries into a single string\n",
    "text = \"\\n\\n\".join(summaries)\n",
    "\n",
    "# Generate a final summary of the entire document\n",
    "final_summary = summarize_chunks([text], max_tokens=512)\n",
    "\n",
    "# Print the final summary to the console\n",
    "print(final_summary[0])\n",
    "\n",
    "# Write the final summary to a file\n",
    "with open(\"output.txt\", \"w\") as file:\n",
    "    file.write(final_summary[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551de19b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
